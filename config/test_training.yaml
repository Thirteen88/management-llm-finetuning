# Test Training Configuration - Small Model for Quick Testing

model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  load_in_4bit: true
  use_flash_attention: false

lora:
  enabled: true
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  bias: "none"

training:
  num_train_epochs: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 2.0e-4
  warmup_ratio: 0.03
  weight_decay: 0.001
  max_grad_norm: 1.0
  optim: "adamw_torch"

data:
  train_path: "data/processed/train.parquet"
  validation_path: "data/processed/validation.parquet"
  max_seq_length: 512
  preprocessing_num_workers: 2

output:
  output_dir: "models/test-checkpoint"
  logging_steps: 5
  save_steps: 20
  eval_steps: 20
  save_total_limit: 2
  save_strategy: "steps"
  evaluation_strategy: "steps"

hardware:
  device: "cuda"
  bf16: true
  fp16: false

inference:
  temperature: 0.7
  top_p: 0.9
  max_new_tokens: 256
